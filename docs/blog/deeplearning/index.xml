<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deeplearning on 巴别之塔</title>
    <link>https://lqhl.github.io/blog/deeplearning/</link>
    <description>Recent content in deeplearning on 巴别之塔</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <copyright>Copyright © 2021, lqhl.</copyright>
    <lastBuildDate>Mon, 21 Dec 2020 23:07:35 +0800</lastBuildDate><atom:link href="https://lqhl.github.io/blog/deeplearning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>深度学习自动编译和优化技术调研</title>
      <link>https://lqhl.github.io/a-survey-on-deep-learning-compiler-and-optimization/</link>
      <pubDate>Mon, 21 Dec 2020 23:07:35 +0800</pubDate>
      
      <guid>https://lqhl.github.io/a-survey-on-deep-learning-compiler-and-optimization/</guid>
      <description>在墨奇科技的基础架构组，为了解计算机系统方向的最新研究热点，我们每年会组织学习系统界两大顶级会议 OSDI/SOSP 的论文。在 OSDI&#39;20 中，我们看到了多篇关于深度学习自动编译和优化的论文，产生了兴趣，于是把从陈天奇的 TVM 开始的系列论文梳理了一下。
研究背景和动机 深度学习在我们的日常生活中无处不在。深度学习模型可以识别图像，处理自然语言，甚至在一些具有挑战性的策略游戏中击败人类。当前的深度学习框架，如 TensorFlow1、MXNet2 和 PyTorch3，支持在一些 GPU 设备上的深度学习模型加速，这种支持依赖于高度特化、由硬件产商提供的张量算子库（比如 NVIDIA 的 cuDNN）。对于一个张量算子，存在许多逻辑上等效的实现，但由于线程、内存重用、流水线和其他硬件因素的差异，这些实现在性能上会有很大差异。为了优化张量算子，程序员必须从这些逻辑等效的实现中选择性能最好的。这些算子级别的优化需要大量的手动调整，非常的专业和不透明，而且无法轻松地跨硬件设备移植。因此，一个深度学习框架如果想要支持不同的硬件后端，需要大量的工程工作。即使在当前受支持的硬件上，开发深度学习框架和模型也受到库中优化算子集合的限制，从而阻止了可能产生不受支持的算子的优化（例如计算图优化和算子融合）。
从云服务器到自动驾驶汽车和嵌入式设备，将智能应用程序部署到各种各样的设备的需求不断增长。由于硬件特性的多样性，存在 CPU、GPU、ASIC（如 TPC 和 NPU）、FPGA 等不同类型的硬件，将深度学习模型映射到这些硬件设备变得很复杂。这些硬件的设计目标在内存组织、计算功能单元等方面都有很大的不同。深度学习框架依靠计算图的中间表示来实现优化，例如自动微分和动态内存管理。但是，计算图级别的优化通常过于高级，无法处理特定硬件后端的算子级转换。
为了在不同的硬件后端上同时实现计算图级别和算子级别的优化，让深度学习计算被更广泛的应用，我们需要一套自动化的针对不同硬件后端的深度学习编译技术。
代表性框架 这节我们介绍 TVM 和这个领域中的其它框架。
TVM 为了解决以上问题，陈天奇等人提出了 TVM4，第一个端到端的深度学习自动编译和代码生成方法。TVM 允许将高级框架（如 TensorFlow、MXNet、PyTorch 等）专用的深度学习网络部署到多种硬件后端上（包括 CPU、GPU 和基于 FPGA 的加速器）。在设计上，TVM 结合了内存访问、线程模式和新的硬件元语，建立一个足够大的搜索空间，保证可能的人工工程优化全部包含在这个搜索空间里面。TVM 通过快速的搜索这个搜索空间，生成可部署代码。其性能可与当前最优的硬件供应商库相比，且可适应新型专用加速器后端。
TVM 是一个由开源社区共同维护的开源项目，它仍然在持续的进步中。2018 年，陈天奇等人提出了 AutoTVM5，旨在通过机器学习来编译优化深度学习系统底层算子。当 TVM 建立了足够大的搜索空间后，剩下的问题是如何在几十亿的可能性里面去选择比较好的实现。这里有几种常见的做法。传统的高性能计算库如会采用自动整定（auto tuning），也就是把可能的参数都尝试一遍。这样做的潜在问题是空间太大之后枚举开销过高。另外一种常见的做法是类似于数据库做查询优化的做法，针对程序建立一个代价估价函数，然后利用估价函数来搜索。这个做法可能碰到的主要问题是估价函数不一定可以估计准确，并且针对每个新的硬件特性必须要重新设计估价函数。AutoTVM 利用机器学习来学习程序空间的代价估价函数。具体地说， 探索程序在一开始会随机地选取一些设定，直接到硬件上面去运行生成的代码，再通过得到的反馈数据来更新程序代价估计函数。这里面比较有趣的一点是模型的可迁移性。因为真正的深度学习系统需要优化许多不一样输入类型和输入形状的算子。一个可迁移的模型可以通过学习已经看到过的算子优化记录来预测新的目标的代价，导致最后的搜索时间可以大幅降低。
Relay6 是一种功能多样的编程语言，TVM 用 Relay 作为深度学习模型的中间表示（IR, intermediate representation）。Relay 支持代数数据类型、闭包、控制流和递归，从而可以直接表示比基于计算图的中间表示更复杂的模型。Relay 还包括一种使用类型关系的依赖类型的形式，来处理对参数形状有复杂的要求的操作符的形状分析。Relay 在设计上是可扩展的，这使得机器学习的研究人员和实践者可以很容易地开发新的大型程序转换和优化。
郑怜悯等人在 TVM 的基础上实现了 Ansor7。Ansor 主要解决了 TVM 中的两个问题：
 如何自动化的构造一个更大的搜索空间？Ansor 使用了一个层次化的搜索空间； 如何更有效的进行搜索？Ansor 在搜索过程中增加了采样，先对完整的程序进行采样然后再调整，提高了搜索效率。  其它框架 Rammer Rammer8 是微软发布的一个针对深度神经网络的自动编译框架。Rammer 针对的主要是并行计算能力较强的 GPU 和 ASIC 神经网络加速器。在神经网络中，有两个层级可以通过并行来加速，第一层是在神经网络的计算图上，可以将相互之间独立的算子并行化；第二层是在一个算子的内部，比如矩阵乘法，可以使用并行计算加速。Rammer 的特点是提供了对算子和硬件的两种抽象，统一的对两层级的并行进行调度，提高了并行编排的效率。作者在 NVIDIA GPU、AMD GPU 和 GraphCore IPU 上进行了实验，其中在 NVIDIA GPU 上的实验表明，Rammer 在部分模型上的效果超越了 TensorFlow、TVM 和 TensorRT。</description>
    </item>
    
  </channel>
</rss>
